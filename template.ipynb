{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNe/62nR2kLRWKL5TMzF52",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ameyanator/data-science/blob/main/template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wyBGieSmxQSz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function for comparing different approaches\n",
        "def score_dataset(X_train, X_valid, y_train, y_valid, model, scorer):\n",
        "    model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_valid)\n",
        "    return scorer(y_valid, preds)"
      ],
      "metadata": {
        "id": "Sgk0UjvoxZP1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find columns which have data in train set but not validation set\n",
        "def seperate_good_bad_columns_categorical(X_train, X_valid):\n",
        "  # Categorical columns in the training data\n",
        "  object_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n",
        "\n",
        "  # Columns that can be safely ordinal encoded\n",
        "  good_label_cols = [col for col in object_cols if\n",
        "                    set(X_valid[col]).issubset(set(X_train[col]))]\n",
        "\n",
        "  # Problematic columns that will be dropped from the dataset\n",
        "  bad_label_cols = list(set(object_cols)-set(good_label_cols))\n",
        "\n",
        "  print('Categorical columns that will be ordinal encoded:', good_label_cols)\n",
        "  print('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)\n",
        "  return good_label_cols, bad_label_cols"
      ],
      "metadata": {
        "id": "O6Lc2-hlxa4o"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_high_low_cardinality_cols(X_train):\n",
        "  object_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n",
        "  # Columns that will be one-hot encoded\n",
        "  low_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]\n",
        "\n",
        "  # Columns that will be dropped from the dataset\n",
        "  high_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n",
        "\n",
        "  print('Categorical columns that will be one-hot encoded:', low_cardinality_cols)\n",
        "  print('\\nCategorical columns that will be dropped from the dataset:', high_cardinality_cols)\n",
        "  return high_cardinality_cols, low_cardinality_cols"
      ],
      "metadata": {
        "id": "ZbI_OQ0bxcNJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "# ordinal encode dataset\n",
        "def get_ordinal_encoding(X_train, X_valid):\n",
        "  cols = seperate_good_bad_columns_categorical(X_train, X_valid)\n",
        "  good_label_cols = cols[0]\n",
        "  bad_label_cols = cols[1]\n",
        "  # Drop categorical columns that will not be encoded\n",
        "  label_X_train = X_train.drop(bad_label_cols, axis=1)\n",
        "  label_X_valid = X_valid.drop(bad_label_cols, axis=1)\n",
        "\n",
        "  # Apply ordinal encoder\n",
        "  ordinal_encoder = OrdinalEncoder() # Your code here\n",
        "  label_X_train[good_label_cols] = ordinal_encoder.fit_transform(label_X_train[good_label_cols])\n",
        "  label_X_valid[good_label_cols] = ordinal_encoder.transform(label_X_valid[good_label_cols])\n",
        "  return label_X_train, label_X_valid"
      ],
      "metadata": {
        "id": "UCpfcF21xdip"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "#one hot encoding of dataset\n",
        "\n",
        "def get_one_hot_encoding(X_train, X_valid):\n",
        "  # Use as many lines of code as you need!\n",
        "  cols = find_high_low_cardinality_cols(X_train)\n",
        "  high_cardinality_cols = cols[0]\n",
        "  low_cardinality_cols = cols[1]\n",
        "  OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "  OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))\n",
        "  OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_cardinality_cols]))\n",
        "\n",
        "  # One-hot encoding removed index; put it back\n",
        "  OH_cols_train.index = X_train.index\n",
        "  OH_cols_valid.index = X_valid.index\n",
        "\n",
        "  object_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n",
        "  # Remove categorical columns (will replace with one-hot encoding)\n",
        "  num_X_train = X_train.drop(object_cols, axis=1)\n",
        "  num_X_valid = X_valid.drop(object_cols, axis=1)\n",
        "\n",
        "  # Add one-hot encoded columns to numerical features\n",
        "  OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
        "  OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
        "\n",
        "  # Ensure all columns have string type\n",
        "  OH_X_train.columns = OH_X_train.columns.astype(str)\n",
        "  OH_X_valid.columns = OH_X_valid.columns.astype(str)\n",
        "  return OH_X_train, OH_X_valid\n"
      ],
      "metadata": {
        "id": "Qq_tIDXpxfPi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get unique entries per columns\n",
        "def get_unique_entries_per_column(X_train):\n",
        "  object_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n",
        "  # Get number of unique entries in each column with categorical data\n",
        "  object_nunique = list(map(lambda col: X_train[col].nunique(), object_cols))\n",
        "  d = dict(zip(object_cols, object_nunique))\n",
        "\n",
        "  # Print number of unique entries by column, in ascending order\n",
        "  sorted(d.items(), key=lambda x: x[1])"
      ],
      "metadata": {
        "id": "pbCq_56Hxgn4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drop cols with categorical data\n",
        "def drop_cols_with_categorical_data(X_train, X_valid):\n",
        "  # Fill in the lines below: drop columns in training and validation data\n",
        "  drop_X_train = X_train.select_dtypes(exclude=['object'])\n",
        "  drop_X_valid = X_valid.select_dtypes(exclude=['object'])\n",
        "  return drop_X_train, drop_X_valid"
      ],
      "metadata": {
        "id": "VswrsAAqxiJT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cols with missing values\n",
        "def get_cols_with_missing_values(X_train):\n",
        "  # Shape of training data (num_rows, num_columns)\n",
        "  print(\"Shape of X_train: {}\".format(X_train.shape))\n",
        "\n",
        "  # Number of missing values in each column of training data\n",
        "  missing_val_count_by_column = (X_train.isnull().sum())\n",
        "  print(missing_val_count_by_column[missing_val_count_by_column > 0])"
      ],
      "metadata": {
        "id": "eZYssD5ZzXP3"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drop cols with missing data\n",
        "def drop_cols_with_missing_data(X_train, X_valid):\n",
        "  # Get names of columns with missing values\n",
        "  cols_with_missing = [col for col in X_train.columns\n",
        "                      if X_train[col].isnull().any()]\n",
        "\n",
        "  # Drop columns in training and validation data\n",
        "  reduced_X_train = X_train.drop(cols_with_missing, axis=1)\n",
        "  reduced_X_valid = X_valid.drop(cols_with_missing, axis=1)\n",
        "  return reduced_X_train, reduced_X_valid"
      ],
      "metadata": {
        "id": "VXbLWr7mz7Q6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "#impute cols with missing data\n",
        "def impute_cols_with_missing_data(X_train, X_valid, strategy):\n",
        "  imputer = SimpleImputer(strategy=strategy)  # Or use other strategies like 'median', 'most_frequent', etc.\n",
        "  imputed_X_train = pd.DataFrame(imputer.fit_transform(X_train))\n",
        "  imputed_X_valid = pd.DataFrame(imputer.transform(X_valid))\n",
        "\n",
        "  imputed_X_train.columns = X_train.columns\n",
        "  imputed_X_valid.columns = X_valid.columns"
      ],
      "metadata": {
        "id": "UPKiCly11P72"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}